{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmFvykuQV-WH",
        "outputId": "2c61a040-b315-4e49-f29e-2d780f295a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting wikipedia-api\n",
            "  Downloading wikipedia_api-0.7.1.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2024.8.30)\n",
            "Building wheels for collected packages: wikipedia-api\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.7.1-py3-none-any.whl size=14346 sha256=e8ddac7deaf817a80e15799d1f6f7ecf0d6e18d512236cd23072f89e3060aca4\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/96/18/b9201cc3e8b47b02b510460210cfd832ccf10c0c4dd0522962\n",
            "Successfully built wikipedia-api\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.7.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#Install all necessery packages\n",
        "!pip install nltk wikipedia-api\n",
        "import nltk\n",
        "import wikipediaapi\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "#Download all necessery data\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task-1: Lesk disambiguation"
      ],
      "metadata": {
        "id": "djIuK3eFZnX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Natural Language Toolkit: Word Sense Disambiguation Algorithms\n",
        "#\n",
        "# Authors: Liling Tan <alvations@gmail.com>,\n",
        "#          Dmitrijs Milajevs <dimazest@gmail.com>\n",
        "#\n",
        "# Copyright (C) 2001-2024 NLTK Project\n",
        "# URL: <https://www.nltk.org/>\n",
        "# For license information, see LICENSE.TXT\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def lesk(context_sentence, ambiguous_word, pos=None, synsets=None, lang=\"eng\"):\n",
        "    \"\"\"Return a synset for an ambiguous word in a context.\n",
        "\n",
        "    :param iter context_sentence: The context sentence where the ambiguous word\n",
        "         occurs, passed as an iterable of words.\n",
        "    :param str ambiguous_word: The ambiguous word that requires WSD.\n",
        "    :param str pos: A specified Part-of-Speech (POS).\n",
        "    :param iter synsets: Possible synsets of the ambiguous word.\n",
        "    :param str lang: WordNet language.\n",
        "    :return: ``lesk_sense`` The Synset() object with the highest signature overlaps.\n",
        "\n",
        "    This function is an implementation of the original Lesk algorithm (1986) [1].\n",
        "\n",
        "    Usage example::\n",
        "\n",
        "        >>> lesk(['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.'], 'bank', 'n')\n",
        "        Synset('savings_bank.n.02')\n",
        "\n",
        "    [1] Lesk, Michael. \"Automatic sense disambiguation using machine\n",
        "    readable dictionaries: how to tell a pine cone from an ice cream\n",
        "    cone.\" Proceedings of the 5th Annual International Conference on\n",
        "    Systems Documentation. ACM, 1986.\n",
        "    https://dl.acm.org/citation.cfm?id=318728\n",
        "    \"\"\"\n",
        "\n",
        "    context = set(context_sentence)\n",
        "    if synsets is None:\n",
        "        synsets = wordnet.synsets(ambiguous_word, lang=lang)\n",
        "\n",
        "    if pos:\n",
        "        synsets = [ss for ss in synsets if str(ss.pos()) == pos]\n",
        "\n",
        "    if not synsets:\n",
        "        return None\n",
        "\n",
        "    _, sense = max(\n",
        "        (len(context.intersection(ss.definition().split())), ss) for ss in synsets\n",
        "    )\n",
        "\n",
        "    return sense"
      ],
      "metadata": {
        "id": "vw9vFMnHZluL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I was awarded a chair in computer science.\"\n",
        "target_word = \"chair\"\n",
        "correct_sense = lesk(sentence, target_word)\n",
        "print(correct_sense)\n",
        "print(correct_sense.definition())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqK-1SRjZ69K",
        "outputId": "f07518c2-bada-45e5-cce5-779480e66a2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('chair.v.01')\n",
            "act or preside as chair, as of an academic department in a university\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task-2: Wikipedia base disambiguation using Lesk algorithm"
      ],
      "metadata": {
        "id": "o2Gnbt6nb3q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#modified lesk implementation\n",
        "def lesk_similarity(context_sentence, sense_defination):\n",
        "  context_tokens = set(word_tokenize(context_sentence.lower()))\n",
        "  sense_def_tokens = set(word_tokenize(sense_defination.lower()))\n",
        "  overlap = len(sense_def_tokens.intersection(context_tokens))\n",
        "  return overlap"
      ],
      "metadata": {
        "id": "V1TSwf6Cm0J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#search the target word on wikipedia\n",
        "agent = \"Project_Task_1_2_3/1.0 (mdrabiulhasan7890@gmail.com)\"\n",
        "wiki_wiki = wikipediaapi.Wikipedia(language='en',\n",
        "                                   extract_format=wikipediaapi.ExtractFormat.WIKI,\n",
        "                                   user_agent=agent)\n",
        "\n",
        "def search_targetword_on_wiki(word):\n",
        "  main_page = wiki_wiki.page(f\"{word} (disambiguation)\")\n",
        "  senses = []\n",
        "  if main_page.exists():\n",
        "    for link in main_page.links.values():\n",
        "      sense_page = wiki_wiki.page(link.title)\n",
        "      if sense_page.exists() and sense_page.summary:\n",
        "        senses.append((link.title, sense_page.text))\n",
        "\n",
        "  else:\n",
        "      main_page = wiki_wiki.page(word)\n",
        "      if main_page.exists() and main_page.summary:\n",
        "        senses.append((word, main_page.text))\n",
        "  return senses\n",
        "\n",
        "\n",
        "#Use wiki for lexical resourse (sense, text) for disambiguation\n",
        "def wiki_disambiguation(sentence, target_word):\n",
        "  senses = search_targetword_on_wiki(target_word)\n",
        "  max_overlap = 0\n",
        "  best_sense = None\n",
        "  for title, text in senses:\n",
        "    overlap = lesk_similarity(sentence, text)\n",
        "    if overlap > max_overlap:\n",
        "      max_overlap = overlap\n",
        "      best_sense = (title, text)\n",
        "  return best_sense\n"
      ],
      "metadata": {
        "id": "kSDwrJskcYV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install summa\n",
        "from summa import summarizer\n",
        "\n",
        "sentence = \"I was awarded a chair in computer science.\"\n",
        "target_word = \"chair\"\n",
        "\n",
        "wiki_based_best_sense = wiki_disambiguation(sentence, target_word)\n",
        "\n",
        "text = wiki_based_best_sense[1]\n",
        "summary = summarizer.summarize(text, ratio=0.01)\n",
        "\n",
        "print(\"Best Sense:\", wiki_based_best_sense[0])\n",
        "print(\"Summary:\", summary)"
      ],
      "metadata": {
        "id": "9y8M-FAInZI1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "049c7174-87ec-4555-c6f8-21b31b17cf77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting summa\n",
            "  Downloading summa-1.2.0.tar.gz (54 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from summa) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy>=0.19->summa) (1.26.4)\n",
            "Building wheels for collected packages: summa\n",
            "  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54387 sha256=9c4afa68ffff417565a7580ca81ad05340aeef2ea2c7c6b097abe937bc86abc7\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/ca/c5/4958614cfba88ed6ceb7cb5a849f9f89f9ac49971616bc919f\n",
            "Successfully built summa\n",
            "Installing collected packages: summa\n",
            "Successfully installed summa-1.2.0\n",
            "Best Sense: Chair (academic)\n",
            "Summary: Professor (commonly abbreviated as Prof.) is an academic rank at universities and other post-secondary education and research institutions in most countries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KJ5sBdeU2Eo_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}